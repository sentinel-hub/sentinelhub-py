{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel Hub Statistical API\n",
    "\n",
    "This notebook shows how to use Sentinel Hub Statistical API to obtain aggregated statistical satellite data for areas of interest. For more information about Statistical API please check the [official service documentation](https://docs.sentinel-hub.com/api/latest/api/statistical/).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Imports\n",
    "\n",
    "The tutorial requires additional packages `geopandas`, `matplotlib`, and `seaborn` which are not dependencies of `sentinelhub-py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sentinelhub import (\n",
    "    CRS,\n",
    "    BBox,\n",
    "    DataCollection,\n",
    "    Geometry,\n",
    "    SentinelHubStatistical,\n",
    "    SentinelHubStatisticalDownloadClient,\n",
    "    SHConfig,\n",
    "    parse_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "Process API requires Sentinel Hub account. Please check [configuration instructions](https://sentinelhub-py.readthedocs.io/en/latest/configure.html) about how to set up your Sentinel Hub credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SHConfig()\n",
    "\n",
    "if not config.sh_client_id or not config.sh_client_secret:\n",
    "    print(\"Warning! To use Statistical API, please provide the credentials (OAuth client ID and client secret).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "\n",
    "A helper function that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_to_df(stats_data):\n",
    "    \"\"\"Transform Statistical API response into a pandas.DataFrame\"\"\"\n",
    "    df_data = []\n",
    "\n",
    "    for single_data in stats_data[\"data\"]:\n",
    "        df_entry = {}\n",
    "        is_valid_entry = True\n",
    "\n",
    "        df_entry[\"interval_from\"] = parse_time(single_data[\"interval\"][\"from\"]).date()\n",
    "        df_entry[\"interval_to\"] = parse_time(single_data[\"interval\"][\"to\"]).date()\n",
    "\n",
    "        for output_name, output_data in single_data[\"outputs\"].items():\n",
    "            for band_name, band_values in output_data[\"bands\"].items():\n",
    "                band_stats = band_values[\"stats\"]\n",
    "                if band_stats[\"sampleCount\"] == band_stats[\"noDataCount\"]:\n",
    "                    is_valid_entry = False\n",
    "                    break\n",
    "\n",
    "                for stat_name, value in band_stats.items():\n",
    "                    col_name = f\"{output_name}_{band_name}_{stat_name}\"\n",
    "                    if stat_name == \"percentiles\":\n",
    "                        for perc, perc_val in value.items():\n",
    "                            perc_col_name = f\"{col_name}_{perc}\"\n",
    "                            df_entry[perc_col_name] = perc_val\n",
    "                    else:\n",
    "                        df_entry[col_name] = value\n",
    "\n",
    "        if is_valid_entry:\n",
    "            df_data.append(df_entry)\n",
    "\n",
    "    return pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Statistical API request\n",
    "\n",
    "In the [Process API tutorial](./process_request.ipynb), we have seen how to obtain satellite imagery. Statistical API can be used in a very similar way. The main difference is that the results of Statistical API are aggregated statistical values of satellite data instead of entire images. In many use cases, such values are all that we need. By using Statistical API we can avoid downloading and processing large amounts of satellite data.\n",
    "\n",
    "Let's take the same bounding box of Betsiboka Estuary, used in Process API tutorial, and make a Statistical API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betsiboka_bbox = BBox((46.16, -16.15, 46.51, -15.58), CRS.WGS84)\n",
    "\n",
    "rgb_evalscript = \"\"\"\n",
    "//VERSION=3\n",
    "\n",
    "function setup() {\n",
    "  return {\n",
    "    input: [\n",
    "      {\n",
    "        bands: [\n",
    "          \"B02\",\n",
    "          \"B03\",\n",
    "          \"B04\",\n",
    "          \"dataMask\"\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    output: [\n",
    "      {\n",
    "        id: \"rgb\",\n",
    "        bands: [\"R\", \"G\", \"B\"]\n",
    "      },\n",
    "      {\n",
    "        id: \"dataMask\",\n",
    "        bands: 1\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    return {\n",
    "      rgb: [samples.B04, samples.B03, samples.B02],\n",
    "      dataMask: [samples.dataMask]\n",
    "    };\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rgb_request = SentinelHubStatistical(\n",
    "    aggregation=SentinelHubStatistical.aggregation(\n",
    "        evalscript=rgb_evalscript,\n",
    "        time_interval=(\"2020-06-07\", \"2020-06-13\"),\n",
    "        aggregation_interval=\"P1D\",\n",
    "        size=(631, 1047),\n",
    "    ),\n",
    "    input_data=[SentinelHubStatistical.input_data(DataCollection.SENTINEL2_L1C, maxcc=0.8)],\n",
    "    bbox=betsiboka_bbox,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will send the request to Sentinel Hub service and obtain results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rgb_stats = rgb_request.get_data()[0]\n",
    "\n",
    "rgb_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained statistical data for pixels for each band and for both available acquisition dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple requests for a collection of geometries\n",
    "\n",
    "The real value of Statistical API shows for use cases where we have a large collection of small geometries, sparsely distributed over a large geographical area, and we would like to obtain statistical information for each of them.\n",
    "\n",
    "In this example, we will take `4` small polygons, each marking a different land cover type, and collect [NDVI](https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index) values for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_gdf = gpd.read_file(\"data/statapi_test.geojson\")\n",
    "\n",
    "polygons_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each polygon, we define a Statistical API request. Requested statistical data will be calculated on `10`-meter resolution and aggregated per day with an available acquisition. We will also request histogram values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_time_interval = \"2020-01-01\", \"2020-12-31\"\n",
    "\n",
    "ndvi_evalscript = \"\"\"\n",
    "//VERSION=3\n",
    "\n",
    "function setup() {\n",
    "  return {\n",
    "    input: [\n",
    "      {\n",
    "        bands: [\n",
    "          \"B04\",\n",
    "          \"B08\",\n",
    "          \"dataMask\"\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    output: [\n",
    "      {\n",
    "        id: \"ndvi\",\n",
    "        bands: 1\n",
    "      },\n",
    "      {\n",
    "        id: \"dataMask\",\n",
    "        bands: 1\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    return {\n",
    "      ndvi: [index(samples.B08, samples.B04)],\n",
    "      dataMask: [samples.dataMask]\n",
    "    };\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "aggregation = SentinelHubStatistical.aggregation(\n",
    "    evalscript=ndvi_evalscript, time_interval=yearly_time_interval, aggregation_interval=\"P1D\", resolution=(10, 10)\n",
    ")\n",
    "\n",
    "input_data = SentinelHubStatistical.input_data(DataCollection.SENTINEL2_L2A)\n",
    "\n",
    "histogram_calculations = {\"ndvi\": {\"histograms\": {\"default\": {\"nBins\": 20, \"lowEdge\": -1.0, \"highEdge\": 1.0}}}}\n",
    "\n",
    "ndvi_requests = []\n",
    "\n",
    "for geo_shape in polygons_gdf.geometry.values:\n",
    "    request = SentinelHubStatistical(\n",
    "        aggregation=aggregation,\n",
    "        input_data=[input_data],\n",
    "        geometry=Geometry(geo_shape, crs=CRS(polygons_gdf.crs)),\n",
    "        calculations=histogram_calculations,\n",
    "        config=config,\n",
    "    )\n",
    "    ndvi_requests.append(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of triggering download for each request separately, we can pass request objects together to a download client object, which will execute them in parallel using multiple threads. This way the download process will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "download_requests = [ndvi_request.download_list[0] for ndvi_request in ndvi_requests]\n",
    "\n",
    "client = SentinelHubStatisticalDownloadClient(config=config)\n",
    "\n",
    "ndvi_stats = client.download(download_requests)\n",
    "\n",
    "len(ndvi_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this data into a tabular form by transforming it into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_dfs = [stats_to_df(polygon_stats) for polygon_stats in ndvi_stats]\n",
    "\n",
    "for df, land_type in zip(ndvi_dfs, polygons_gdf[\"land_type\"].values):\n",
    "    df[\"land_type\"] = land_type\n",
    "\n",
    "ndvi_df = pd.concat(ndvi_dfs)\n",
    "\n",
    "ndvi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows time series of mean values, buffered by standard deviation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "for idx, land_type in enumerate(polygons_gdf[\"land_type\"].values):\n",
    "    series = ndvi_df[ndvi_df[\"land_type\"] == land_type]\n",
    "\n",
    "    series.plot(ax=ax, x=\"interval_from\", y=\"ndvi_B0_mean\", color=f\"C{idx}\", label=land_type)\n",
    "\n",
    "    ax.fill_between(\n",
    "        series.interval_from.values,\n",
    "        series[\"ndvi_B0_mean\"] - series[\"ndvi_B0_stDev\"],\n",
    "        series[\"ndvi_B0_mean\"] + series[\"ndvi_B0_stDev\"],\n",
    "        color=f\"C{idx}\",\n",
    "        alpha=0.3,\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot histograms for a certain timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_INDEX = 2\n",
    "\n",
    "plot_data = []\n",
    "timestamp = None\n",
    "for idx, stats in enumerate(ndvi_stats):\n",
    "    bins = stats[\"data\"][TIMESTAMP_INDEX][\"outputs\"][\"ndvi\"][\"bands\"][\"B0\"][\"histogram\"][\"bins\"]\n",
    "    timestamp = stats[\"data\"][TIMESTAMP_INDEX][\"interval\"][\"from\"].split(\"T\")[0]\n",
    "\n",
    "    counts = [value[\"count\"] for value in bins]\n",
    "    total_counts = sum(counts)\n",
    "    counts = [round(100 * count / total_counts) for count in counts]\n",
    "\n",
    "    bin_size = bins[1][\"lowEdge\"] - bins[0][\"lowEdge\"]\n",
    "    splits = [value[\"lowEdge\"] + bin_size / 2 for value in bins]\n",
    "\n",
    "    data = []\n",
    "    for count, split in zip(counts, splits):\n",
    "        data.extend([split] * count)\n",
    "    plot_data.append(np.array(data))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax = sns.violinplot(data=plot_data, ax=ax)\n",
    "ax.set(xticklabels=polygons_gdf[\"land_type\"].values)\n",
    "plt.xlabel(\"Land types\", fontsize=15)\n",
    "plt.ylabel(f\"NDVI for {timestamp}\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce service processing costs\n",
    "\n",
    "In case of large-scale processing, it becomes important, how many processing units we spend making Statistical API requests. We can decrease this number if we write an evalscript that outputs integer values instead of floats.\n",
    "\n",
    "In this example, we will download statistics for all Sentinel-2 L2A bands, cloud masks and probabilities, and a few remote sensing indices. Such a collection of values would typically be used as an input to a machine learning model.\n",
    "\n",
    "The following evalscript will:\n",
    "\n",
    "- request band values in digital numbers, instead of reflectances,\n",
    "- use `toUINT` function to convert values from indices into integers that will fit into `SampleType.UINT16`,\n",
    "- mask pixels for which cloud mask `CLM` indicates that they contain clouds. Such pixels will not be included in statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(\"./data/statapi_evalscript.js\", \"r\") as fp:\n",
    "    features_evalscript = fp.read()\n",
    "\n",
    "print(features_evalscript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create requests for all `4` polygons. Additionally, we will request statistics for `5th`, `50th` and `95th` percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation = SentinelHubStatistical.aggregation(\n",
    "    evalscript=features_evalscript, time_interval=yearly_time_interval, aggregation_interval=\"P1D\", resolution=(10, 10)\n",
    ")\n",
    "\n",
    "calculations = {\"default\": {\"statistics\": {\"default\": {\"percentiles\": {\"k\": [5, 50, 95]}}}}}\n",
    "\n",
    "features_requests = []\n",
    "for geo_shape in polygons_gdf.geometry.values:\n",
    "    request = SentinelHubStatistical(\n",
    "        aggregation=aggregation,\n",
    "        input_data=[SentinelHubStatistical.input_data(DataCollection.SENTINEL2_L2A)],\n",
    "        geometry=Geometry(geo_shape, crs=CRS(polygons_gdf.crs)),\n",
    "        calculations=calculations,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    features_requests.append(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "download_requests = [request.download_list[0] for request in features_requests]\n",
    "\n",
    "client = SentinelHubStatisticalDownloadClient(config=config)\n",
    "\n",
    "features_stats = client.download(download_requests)\n",
    "\n",
    "len(features_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert service response into a dataframe. Compared to the previous example this dataframe will have more columns, as we requested more types of features, and fewer rows, as in some cases all pixels contained clouds and were masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dfs = [stats_to_df(polygon_stats) for polygon_stats in features_stats]\n",
    "\n",
    "for df, land_type in zip(features_dfs, polygons_gdf[\"land_type\"].values):\n",
    "    df[\"land_type\"] = land_type\n",
    "\n",
    "features_df = pd.concat(features_dfs)\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can rescale values back to the correct scale. The following will:\n",
    "\n",
    "- convert statistical values for bands from digital numbers to reflectances,\n",
    "- apply an inverse transformation of `toUINT` function on statistical values for indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = DataCollection.SENTINEL2_L2A.bands\n",
    "INDICES = [\"NDVI\", \"NDVI_RE1\", \"NBSI\", \"CLP\"]\n",
    "STATISTICAL_QUANTITIES = [\"mean\", \"min\", \"max\", \"stDev\", \"percentiles_5.0\", \"percentiles_50.0\", \"percentiles_95.0\"]\n",
    "\n",
    "for band in BANDS:\n",
    "    for stat in STATISTICAL_QUANTITIES:\n",
    "        column_name = f\"bands_{band.name}_{stat}\"\n",
    "        column = features_df[column_name]\n",
    "\n",
    "        column = column / 10000.0\n",
    "\n",
    "        features_df[column_name] = column\n",
    "\n",
    "for index in INDICES:\n",
    "    for stat in STATISTICAL_QUANTITIES:\n",
    "        column_name = f\"indices_{index}_{stat}\"\n",
    "        column = features_df[column_name]\n",
    "\n",
    "        if stat == \"stDev\":\n",
    "            column = column / 5000.0\n",
    "        else:\n",
    "            column = (column - 5000.0) / 5000.0\n",
    "\n",
    "        features_df[column_name] = column\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot NDVI time series. The number of irregularities in the series has decreased because we masked out cloudy pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "for idx, land_type in enumerate(polygons_gdf[\"land_type\"].values):\n",
    "    series = features_df[features_df[\"land_type\"] == land_type]\n",
    "\n",
    "    series.plot(ax=ax, x=\"interval_from\", y=\"indices_NDVI_mean\", color=f\"C{idx}\", label=land_type)\n",
    "\n",
    "    ax.fill_between(\n",
    "        series.interval_from.values,\n",
    "        series[\"indices_NDVI_mean\"] - series[\"indices_NDVI_stDev\"],\n",
    "        series[\"indices_NDVI_mean\"] + series[\"indices_NDVI_stDev\"],\n",
    "        color=f\"C{idx}\",\n",
    "        alpha=0.3,\n",
    "    );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
